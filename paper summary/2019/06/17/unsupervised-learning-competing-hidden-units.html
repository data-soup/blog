<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Paper Summary. Unsupervised learning by competing hidden units | data soup</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Paper Summary. Unsupervised learning by competing hidden units" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is a summary of Unsupervised learning by competing hidden units." />
<meta property="og:description" content="This is a summary of Unsupervised learning by competing hidden units." />
<link rel="canonical" href="https://data-soup.github.io/blog/paper%20summary/2019/06/17/unsupervised-learning-competing-hidden-units.html" />
<meta property="og:url" content="https://data-soup.github.io/blog/paper%20summary/2019/06/17/unsupervised-learning-competing-hidden-units.html" />
<meta property="og:site_name" content="data soup" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-17T16:00:55-05:00" />
<script type="application/ld+json">
{"description":"This is a summary of Unsupervised learning by competing hidden units.","headline":"Paper Summary. Unsupervised learning by competing hidden units","dateModified":"2019-06-17T16:00:55-05:00","datePublished":"2019-06-17T16:00:55-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://data-soup.github.io/blog/paper%20summary/2019/06/17/unsupervised-learning-competing-hidden-units.html"},"url":"https://data-soup.github.io/blog/paper%20summary/2019/06/17/unsupervised-learning-competing-hidden-units.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://data-soup.github.io/blog/feed.xml" title="data soup" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Paper Summary. Unsupervised learning by competing hidden units | data soup</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Paper Summary. Unsupervised learning by competing hidden units" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is a summary of Unsupervised learning by competing hidden units." />
<meta property="og:description" content="This is a summary of Unsupervised learning by competing hidden units." />
<link rel="canonical" href="https://data-soup.github.io/blog/paper%20summary/2019/06/17/unsupervised-learning-competing-hidden-units.html" />
<meta property="og:url" content="https://data-soup.github.io/blog/paper%20summary/2019/06/17/unsupervised-learning-competing-hidden-units.html" />
<meta property="og:site_name" content="data soup" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-17T16:00:55-05:00" />
<script type="application/ld+json">
{"description":"This is a summary of Unsupervised learning by competing hidden units.","headline":"Paper Summary. Unsupervised learning by competing hidden units","dateModified":"2019-06-17T16:00:55-05:00","datePublished":"2019-06-17T16:00:55-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://data-soup.github.io/blog/paper%20summary/2019/06/17/unsupervised-learning-competing-hidden-units.html"},"url":"https://data-soup.github.io/blog/paper%20summary/2019/06/17/unsupervised-learning-competing-hidden-units.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://data-soup.github.io/blog/feed.xml" title="data soup" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">data soup</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Paper Summary. Unsupervised learning by competing hidden units</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-06-17T16:00:55-05:00" itemprop="datePublished">
        Jun 17, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#paper summary">paper summary</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This is a summary of <a href="https://www.pnas.org/content/pnas/116/16/7723.full.pdf">Unsupervised learning by competing hidden units</a>.</p>

<hr />

<p>This paper introduces a novel unsupervised learning technique. There’s (almost) no backprop and the model isn’t trained for a particular task. The two authors, coming from neuroscience and computer science backgrounds based this work on two biological observations:</p>

<p>1- Synapses changes are local:</p>

<blockquote>
  <p>In biology, the synapse update depends on the activities of the presynaptic cell and the postsynaptic cell and perhaps on some global variables such as how well the task was carried out. (page 1)</p>
</blockquote>

<p>The weight of a cell between A and B trained with backpropagation not only depends on the activity of A and B but also on the previous layer’s activity and the training labels. So it doesn’t depend on A, B activity but other potentially any other neurons in the network. This is inspired by <a href="https://en.wikipedia.org/wiki/Hebbian_theory">Hebb</a>’s idea.</p>

<p>2- Animals learn without labeled data and fewer data than neural networks trained with backpropagation:</p>

<blockquote>
  <p>Second, higher animals require extensive sensory experience to tune the early […] visual system into an adult system. This experience is believed to be predominantly observational, with few or no labels, so that there is no explicit task. (page 1)</p>
</blockquote>

<h3 id="unsupervised-local-training">Unsupervised local training</h3>

<p>Authors managed to train their model on MNIST and CIFAR-10 with only forward passes, meaning:</p>
<ul>
  <li>This technique is less computationally demanding, its computational complexity is comparable to the computational complexity of the forward pass in backpropagation (<a href="https://youtu.be/4lY-oAY0aQU?t=1581">source</a>).</li>
  <li>Doesn’t require to train the model on a given task to make meaningful representation from the data.</li>
</ul>

<p>The blue rectangles are the authors “biological learning algorithm”. First, the data is going through it, without any label or any indication on the task it’ll be used for. Once trained a fully connected network is appended on top of it in order to specialize the model and make the desired predictions. This part is trained using backpropagation.</p>

<p><img src="competing-hidden-units/fig01-training-schema.png" alt="figure 01, page 02" /></p>

<p>Usually to compute the activity of the hidden layer <code class="highlighter-rouge">hμ</code>, we project the input <code class="highlighter-rouge">vi</code> on it by multiplying it with a matrix <code class="highlighter-rouge">Wμi</code> and then apply non-linearity. In this new technique the <code class="highlighter-rouge">hμ</code> activity is computed solving this differential equation:</p>

<p><img src="competing-hidden-units/eq8-learning-rule.png" alt="equation 08, page 04" /></p>

<ul>
  <li><code class="highlighter-rouge">μ</code> is the index of the hidden layer we want to update</li>
  <li><code class="highlighter-rouge">τ</code> is a timescale of the process</li>
  <li><code class="highlighter-rouge">Iμ</code> is the input current</li>
  <li>The second term, the sum of all other hidden layers, introduce competition between neurons. Stronger units will inhibit weaker ones. Without it, all neurons will fire activation when input is shown. Note that this term introduces lateral connections between units since they units within the same layer can be connected to each other.</li>
  <li><code class="highlighter-rouge">r</code> is a ReLU and <code class="highlighter-rouge">winh</code> is a hyperparameter constant.</li>
</ul>

<p>Since training is local and requires only forward passes, this architecture is different from an auto-encoder.</p>

<h3 id="in-action">In action</h3>

<p>In an experiment on MNIST and CIFAR-10, the authors trained 2000 hidden units using their biological technique to find the matrix <code class="highlighter-rouge">Wμi</code>:</p>

<ul>
  <li>Hidden units were initialized with a normal distribution</li>
  <li>Hidden units are trained (again, without explicit task or labels)</li>
  <li>Those units are then frozen and plugged to a perceptron</li>
  <li>The perceptron weights were trained using SGD</li>
</ul>

<p>The training error on MNIST can be seen in the rightmost figure of the image below (BP stands for backpropagation and BIO for the proposed approach). We can see that despite a higher training error, the testing error is very close to the model trained end-to-end.</p>

<p><img src="competing-hidden-units/fig-03-mnist-in-action.png" alt="figure 03, page 05" /></p>

<p>On MNIST, we can see that the features learned by the proposed biological learning algorithm (left figure) are different from the one trained with backpropagation (middle figure).</p>

<blockquote>
  <p>the network learns a distributed representation of the
data over multiple hidden units. This representation, however, is
very different from the representation learned by the network
trained end-to-end, as is clear from comparison of Fig. 3, Left
and Center.</p>
</blockquote>

<p>Similarly for CIFAR-10:</p>

<p><img src="competing-hidden-units/fig-07-cifar-in-action.png" alt="figure 07, page 07" /></p>

<h3 id="tldr">tldr</h3>

<blockquote>
  <p>no top–down propagation of information, the synaptic weights are learned using only bottom-up signals, and the algorithm is agnostic about the task that the network will have to solve eventually in the top layer (page 8)</p>
</blockquote>

<ul>
  <li>A new unsupervised training technique, where the task isn’t defined, the training set goes through the model and is trained without backpropagation. A fully connected perceptron is appended on top, trained with backpropagation with the lower unsupervised submodel is frozen.</li>
  <li>This technique shows poorer but near state of the art generalizations performances on MNIST and CIFAR.</li>
  <li>There’s no forward/backward, each cell is potentially connected to every other, including on its own layer.</li>
</ul>

<hr />

<table>
  <thead>
    <tr>
      <th>Author</th>
      <th>Organization</th>
      <th>Previous work</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-krotov">Dmitry Krotov</a></td>
      <td>MIT, IBM (Watson, Research), Princeton</td>
      <td><a href="https://arxiv.org/abs/1701.00939">Dense Associative Memory Is Robust to Adversarial Inputs</a></td>
    </tr>
    <tr>
      <td><a href="http://pni.princeton.edu/john-hopfield">John J. Hopfield</a></td>
      <td>Princeton Neuroscience Institute</td>
      <td>same</td>
    </tr>
  </tbody>
</table>

<p>Complementary resources:</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=4lY-oAY0aQU">Video presentation</a>  by one of the authors at MIT.</li>
  <li><a href="https://github.com/DimaKrotov/Biological_Learning/blob/master/Unsupervised_learning_algorithm_MNIST.ipynb">Github</a> for reproduction.</li>
  <li><a href="https://www.ibm.com/blogs/research/2019/04/biological-algorithm/">Blog post</a> on IBM’s blog.</li>
</ul>

  </div><a class="u-url" href="/blog/paper%20summary/2019/06/17/unsupervised-learning-competing-hidden-units.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>🍜</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
