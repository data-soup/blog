{
  
    
        "post0": {
            "title": "Overview of One Year of Lottery Ticket Research",
            "content": "Winning tickets were discovered in March 2018 and presented at ICRL the same year. It drawed a lot of attention. It sheds light on yet unknown underlying properties of neural networks and seems to be one of the keys for faster training and smaller models. Overall flipping on the head how we approach neural net architecture design. . Winning tickets in deep learning were mentioned as one of the most important topics of 2019 by Lex Fridman‚Äôs in his Deep Learning State of the Art 2020 (awesome) lecture. This article aims at summarizing what I understood after reading about it. Hope you‚Äôll enjoy it. . Pruning . It is known that DL models have generally heavy computational requirements and can be blocking in particular settings. ResNet, for instance, requires 50M operations for one single inference. They‚Äôve been efforts to reduce the number of parameters with quantization, knowledge distillation and pruning. . Pruning removes the least important weights or channels. Least important can mean the one with the smallest magnitudes or other heuristics. Such a technique is working well and can reduce up to 90% of the weights in a network while preserving most of the original accuracy. While pruning can help to reduce the model‚Äôs size, it won‚Äôt help training it faster. It is generally a post-processing step, after training. Retraining a pruned model won‚Äôt yield the same results as if you prune after training. If it were possible to be able to train the pruned model directly, train faster without sacrificing performances. . But in their paper Jonathan Frankle, Michael Carbin experimentally found that instead of training large networks and then reduce their size we might be able to train smaller networks upfront: . dense, randomly-initialized, feed-forward networks contain subnetworks (‚Äúwinning tickets‚Äù) that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. . . Winning Tickets . In order to find winning tickets, initialization seems to be the key: . When their parameters are randomly reinitialized [‚Ä¶], our winning tickets no longer match the performance of the original network, offering evidence that these smaller networks do not train effectively unless they are appropriately initialized. . They found that we can train a pruned model again after re-initializing the weights with the original model‚Äôs parameters. This gives systematically better results than re-initializing randomly. Doing this process multiple times is called iterative pruning (with no re-init): . 1. Randomly initialize a neural network [with weights Œ∏0] 2. Train the network for j iterations, arriving at parameters Œ∏j 3. Prune [by magnitude] p% of the parameters in Œ∏j , creating a mask m 4. Reset the remaining parameters to their values in Œ∏0 5. Goto 2 . If the subnetwork produced by this technique matches the original network‚Äôs performances, it is called a winning ticket. The following graph represents averaged results of five runs on a LeNet (fully dense) network on the MNIST dataset. This model was pruned in different ways: . [blue] Done with the recipe above | [orange] Same as blue but replace step 4 by ‚ÄúRandomly initialize the remaining parameters‚Äù | [red] Same as the orange line without step 5 | [green] Same as blue without step 5 | . . We can see that step 4 is the key as the green and blue lines are consistently performing better and are trained faster than randomly re-initialized networks. They also found similar results with convolutional networks like VGG and ResNet on MNIST and CIFAR10 (there are many more details in the original paper). . . Pruning Early . But the method above seems to struggle against deeper networks. In a follow-up paper (March 2019), the authors changed slightly the way the remaining parameters are reset (step 4): . Rather than set the weights of a winning ticket to their original initializations, we set them to the weights obtained after a small number of training iterations (late resetting). Using late resetting, we identify the first winning tickets for Resnet-50 on Imagenet. . The graph below plot performances against different levels of sparsity of deep models rewound (iteration at which we reset the weights) with different values. We can see that rewinding at iteration 0 does not perform better than the original network whereas rewinding at higher iteration does: . . Those deeper models were resisting the winning ticket recipe above but found something interesting after looking at their stability: . Stability to pruning: ‚Äúthe distance between the weights of a subnetwork trained in isolation and the weights of the same subnetwork when trained within the larger network‚Äù. Which captures ‚Äúa subnetwork‚Äôs ability to train in isolation and still reach the same destination as the larger network‚Äù. If a neuron is stable it won‚Äôt be much affected by its neighbors disappearing through masking. | Stability to data order: ‚Äúthe distance between the weights of two copies of a subnetwork trained with different data orders‚Äù. Which captures ‚Äú a subnetwork‚Äôs intrinsic ability to consistently reach the same destination despite the gradient noise of SGD‚Äù. | . The table below shows stability for different networks. Warmup means that the learning rate is scheduled to increase slowly during training, possibly reducing the noise of the optimizer. IMP is the original recipe to generate winning tickets: . . We can see that IMP fails at fiding winning tickets in deeper networks without changing the learning rate. We can also see that there‚Äôs a link between performances and the stabilities measures. ‚ÄúWinning tickets are more stable than the random subnetworks‚Äù. . . What about other domains? . So far winning tickets have been tested on the same datasets and on computer vision tasks. One can ask if this isn‚Äôt just drastic overfitting or if the winning tickets transfer at all. . Facebook published a paper (June 2019) tested the winning ticket evaluation and transfer across six visual datasets. For instance, testing generating winning tickets on ImageNet and testing it others (like CIFAR-100): . . They observed that winning tickets generalize across all datasets with (at least) close performances than the original one. And that winning tickets generated on larger datasets generalized better than the other ones, probably due to the number of classes in the original model. Finally, this paper also tested the transfer successfully across different optimizers successfully. . What about other tasks than image classification? Facebook published in parallel a paper (June 2019) that test the winning ticket in reinforcement learning and NLP tasks. . For NLP, we found that winning ticket initializations beat random tickets both for recurrent LSTM models trained on language modeling and Transformer models trained on machine translation. [‚Ä¶] For RL, we found that winning ticket initializations substantially outperformed random tickets on classic control problems and for many, but not all, Atari games. . . TLDR . Many neural networks are over-parameterized | Frankle &amp; Carbin found simple algorithm to find smaller network within larger ones | Those sub-networks are trainable from scratch and can perform at least as well and often better | What makes winning tickets special is still unclear but seems to be a critical step toward a deeper understanding of the underlying properties of neural nets | . . Sources . The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks | Stabilizing the Lottery Ticket Hypothesis | One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers | Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP | .",
            "url": "https://data-soup.github.io/blog/paper%20summary/2020/02/13/winning-ticket-overview.html",
            "relUrl": "/paper%20summary/2020/02/13/winning-ticket-overview.html",
            "date": " ‚Ä¢ Feb 13, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Paper Summary. Unsupervised learning by competing hidden units",
            "content": "This is a summary of Unsupervised learning by competing hidden units. . . This paper introduces a novel unsupervised learning technique. There‚Äôs (almost) no backprop and the model isn‚Äôt trained for a particular task. The two authors, coming from neuroscience and computer science backgrounds based this work on two biological observations: . 1- Synapses changes are local: . In biology, the synapse update depends on the activities of the presynaptic cell and the postsynaptic cell and perhaps on some global variables such as how well the task was carried out. (page 1) . The weight of a cell between A and B trained with backpropagation not only depends on the activity of A and B but also on the previous layer‚Äôs activity and the training labels. So it doesn‚Äôt depend on A, B activity but other potentially any other neurons in the network. This is inspired by Hebb‚Äôs idea. . 2- Animals learn without labeled data and fewer data than neural networks trained with backpropagation: . Second, higher animals require extensive sensory experience to tune the early [‚Ä¶] visual system into an adult system. This experience is believed to be predominantly observational, with few or no labels, so that there is no explicit task. (page 1) . Unsupervised local training . Authors managed to train their model on MNIST and CIFAR-10 with only forward passes, meaning: . This technique is less computationally demanding, its computational complexity is comparable to the computational complexity of the forward pass in backpropagation (source). | Doesn‚Äôt require to train the model on a given task to make meaningful representation from the data. | . The blue rectangles are the authors ‚Äúbiological learning algorithm‚Äù. First, the data is going through it, without any label or any indication on the task it‚Äôll be used for. Once trained a fully connected network is appended on top of it in order to specialize the model and make the desired predictions. This part is trained using backpropagation. . . Usually to compute the activity of the hidden layer hŒº, we project the input vi on it by multiplying it with a matrix WŒºi and then apply non-linearity. In this new technique the hŒº activity is computed solving this differential equation: . . Œº is the index of the hidden layer we want to update | œÑ is a timescale of the process | IŒº is the input current | The second term, the sum of all other hidden layers, introduce competition between neurons. Stronger units will inhibit weaker ones. Without it, all neurons will fire activation when input is shown. Note that this term introduces lateral connections between units since they units within the same layer can be connected to each other. | r is a ReLU and winh is a hyperparameter constant. | . Since training is local and requires only forward passes, this architecture is different from an auto-encoder. . In action . In an experiment on MNIST and CIFAR-10, the authors trained 2000 hidden units using their biological technique to find the matrix WŒºi: . Hidden units were initialized with a normal distribution | Hidden units are trained (again, without explicit task or labels) | Those units are then frozen and plugged to a perceptron | The perceptron weights were trained using SGD | . The training error on MNIST can be seen in the rightmost figure of the image below (BP stands for backpropagation and BIO for the proposed approach). We can see that despite a higher training error, the testing error is very close to the model trained end-to-end. . . On MNIST, we can see that the features learned by the proposed biological learning algorithm (left figure) are different from the one trained with backpropagation (middle figure). . the network learns a distributed representation of the data over multiple hidden units. This representation, however, is very different from the representation learned by the network trained end-to-end, as is clear from comparison of Fig. 3, Left and Center. . Similarly for CIFAR-10: . . tldr . no top‚Äìdown propagation of information, the synaptic weights are learned using only bottom-up signals, and the algorithm is agnostic about the task that the network will have to solve eventually in the top layer (page 8) . A new unsupervised training technique, where the task isn‚Äôt defined, the training set goes through the model and is trained without backpropagation. A fully connected perceptron is appended on top, trained with backpropagation with the lower unsupervised submodel is frozen. | This technique shows poorer but near state of the art generalizations performances on MNIST and CIFAR. | There‚Äôs no forward/backward, each cell is potentially connected to every other, including on its own layer. | . . Author Organization Previous work . Dmitry Krotov | MIT, IBM (Watson, Research), Princeton | Dense Associative Memory Is Robust to Adversarial Inputs | . John J. Hopfield | Princeton Neuroscience Institute | same | . Complementary resources: . Video presentation by one of the authors at MIT. | Github for reproduction. | Blog post on IBM‚Äôs blog. | .",
            "url": "https://data-soup.github.io/blog/paper%20summary/2019/06/17/unsupervised-learning-competing-hidden-units.html",
            "relUrl": "/paper%20summary/2019/06/17/unsupervised-learning-competing-hidden-units.html",
            "date": " ‚Ä¢ Jun 17, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Paper Summary. Stiffness: A New Perspective on Generalization in Neural Networks",
            "content": "This is a summary of Stiffness: A New Perspective on Generalization in Neural Networks. . . Stiffness? . This paper aims at improving our understanding of how neural networks generalize from the point of view of stiffness. The intuition behind stiffness is how a gradient update on one point affects another: . [it] characterizes the amount of correlation between changes in loss on the two due to the application of a gradient update based on one of them. (4.1, Results and discussion) . Stiffness is expressed as the expected sign of the gradients g: . . A weight update that improves the loss for X_1 and X_2 is stiff and characterized as anti-stiff if the loss beneficiate for one of the points and doesn‚Äôt help the other. . . The question is now how do we choose X_1 and X_2. Authors explore two ways: by class membership or by distance. . Stiffness based on class membership . We can look at how a gradient update on a point in class A will affect another point‚Äôs loss belonging to class B. In the paper they craft a *class stiffness matrix`, which is the average of stiffness between each point grouped by class: . . The diagonal of this matrix represent the model‚Äôs within class generalization capability. You can find an example of stiffness class matrix at different steps of the training stage: . . At early stages, the stiffness is high between members of the same classes (hence the red diagonal). The majority of the cells raises their stiffness until reaching the point of overfitting: stiffness reaches 0. . Stiffness as a function distance and learning rate . Stiffness is then studied through the distance lens, they distinguish two kinds of distance: pixel-wise (in the input space) and layerwise (in the representational space). . . The general pattern visible in Figure 9 is that there exists a critical distance within which input data points tend to move together under gradient updates, i.e. have positive stiffness. This holds true for all layers in the network, with the tendency of deeper layers to have smaller stiff domain sizes. . Authors define stiff regions as ‚Äúregions of the data space that move together when a gradient update is applied‚Äù. . . We can see that a higher learning rate increase the size of the stiff regions which suggests that higher learning rates help generalization. . tldr . Stiffness quantify how much gradient update on one group of point affects another | Stiffness is tightly linked to generalization | Stiffness tends to 0 when the system overfit | Higher learning rate increases the area under which points are moving together | . . Author Organization Previous work ¬† . Stanislav Fort | Google AI Resident, Google AI Zurich | The Goldilocks zone: Towards better understanding of neural network loss landscapes | ¬† | . Pawe≈Ç Krzysztof Nowak | Google AI Resident | ¬† | ¬† | . Srini Narayanan | Google AI Resident | ¬† | Points, Paths, and Playscapes: Large-scale Spatial Language Understanding Tasks Set in the Real World | . Complementary resources: . Manifold Mixup: Better Representations by Interpolating Hidden States - https://arxiv.org/abs/1806.05236 (cited in the article) | .",
            "url": "https://data-soup.github.io/blog/paper%20summary/2019/03/28/stiffness-new-perspecive-generalization.html",
            "relUrl": "/paper%20summary/2019/03/28/stiffness-new-perspecive-generalization.html",
            "date": " ‚Ä¢ Mar 28, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "What is Knowledge Distillation?",
            "content": "Knowledge distillation is a fascinating concept, we‚Äôll cover briefly why we need it, how it works. . ‚öñÔ∏è Weight Matters . Today‚Äôs models can be quite large, here are some of the top models for the ImageNet dataset: . Model Weights (millions) Size (32-bits floats) Size (16-bits floats) . MobileNet-224 | 4.3 | 17.2 Mo | 8.6 Mo | . VGG16 | 143.7 | 574.8 Mo | 287.4 Mo | . InceptionV3 | 23.9 | 95.6 Mo | 47.8 Mo | . ResNet-50 | 25.6 | 102.4 Mo | 51.2 Mo | . InceptionResNetV2 | 55.9 | 223.6 Mo | 111.8 Mo | . The models were instantiated via keras.applications module with top layers, the number of parameters are given by summary(). . It seems fair to say that simple computer vision models weigh easily ~100Mo. A hundred Mo just to be able to make an inference isn‚Äôt a viable solution for an end product. A remote API can do the trick, but now your product needs to add encryption, you need to store and upload data, the user needs to have a reliable internet connection to have a decent speed. We can train a narrower network, they‚Äôll probably fit in a small memory. But chances are they won‚Äôt be good enough at extracting complex features. . And we‚Äôre not talking about ensembles. Ensembles are a great way to extract a lot of knowledge from the training data. But at test time it can be too expensive to run a hundred different models in parallel. The knowledge per parameter ratio is quite low. . In the end a model can have great score at training time, but we might want to: lower its size (for embedded systems), increase inference speed or simply reduce complexity. Geoffrey Hinton talks about reducing its ‚Äúmemory foot print‚Äù: . . Many insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction. In large-scale machine learning, we typically use very similar models for the training stage and the deployment stage despite their very different requirements (‚Ä¶) (Distilling the Knowledge in a Neural Network) . Training a smaller model from a larger one is called knowledge distillation. . üß™ Distillation . The authors continue that we are identifying knowledge with the values of the weights which makes it ‚Äúhard to see how we can change the form of the model but keep the same knowledge‚Äù. And remind us that we can see knowledge as a mapping from input to output. . Knowledge distillation‚Äôs goal is to transfer the learning from one performant and heavy teacher to a more compact student. . . To do so, we look at the teacher‚Äôs softmax layer, magnify it and the student learns how to produce them. We need to magnify because the softmax layer will smash down to zero the least probable classes and rises close to one the most probable (like one hot vector). We can also keep the relative probabilities between classes, where a motocycle and a bicycle share more similarities on the softmax layer rather than a book. We can do it by raising the temperature T. . . To transfer knowledge, a student is trained on the soften probabilities (T¬ª1) produced by a larger teacher. When the temperature T is smaller than one, the most expected classes will impact the most the final probability. Similarly, when increasing the temperature the probabilities will be softer/flattened across classes -you can have here an intuition of the influence of temperature on a single exp(). . First the teacher‚Äôs temperature is increased until a certain point. Then the student is trained to copy its teacher‚Äôs soft probabilities. . labrador cow golden retriever moto bike ¬† . 1 | 0 | 0 | 0 | 0 | hard targets | . 0.8 | 10^-5 | 0.2 | 10^-9 | 10^-9 | soft targets (T=1) | . 0.6 | 10^-2 | 0.45 | 10^-4 | 10^-4 | soft targets (T¬ª1) | . Benefits . Training on soft targets has several advantages: more information can be extracted from a single sample, training can be done on fewer examples, no need for labeled data . The softmax of a multi-class classifier will give you higher probabilities for similar images. A rose may have similar soft probabilities with a tulip rather than a labrador. Similarly, two different classes are present in the same image, we might see it on the output. So more information are extracted from each training sample. . This is a consequence from the first point, the model can be trained on fewer training examples than the teacher. The learning is also faster because there are more constraints on the student. It needs to target multiple (soft) outputs rather than a single (hard) one. . Since the student learns from soft targets only, by relative similarities between classes, it can be trained on a unlabelled dataset, using only the master has an on-fly ‚Äúsoft labeler‚Äù. But in practice, the dataset can be the same as the teacher. . Loss . Distillation loss is generally in two forms: matching function values, matching derivatives or both, corresponding to a regression problem with different orders: . Matching function values: tries to minimize the difference between the predictions of the teacher and the student. For a classification task, this is done by using classical cross entropy. . | Matching derivatives: tries to match the values and the derivatives. This is a more efficient approach than before because here we can have full access to the teacher and we are able to measure the impacts of small variations in its inputs. . | . We can also try to increase the influence of the prediction by adding directly the hard loss: . alpha ~= 0.1 KD_loss = alpha * log_loss(y_true, softmax(logits)) + logloss(y_true, softmax(logits/temperature)) . You can see a cool implementation here. . Resources . TTIC Geoffrey Hilton - Dark Knowledge - presentation by the author of the first Knowledge Distillation paper . | IEE Security Symposium, Papernot: Note that the distillation as a counter measure for adversarial examples has been proven to be not effective anymore. . | . .",
            "url": "https://data-soup.github.io/blog/2018/11/22/knowledge-distillation.html",
            "relUrl": "/2018/11/22/knowledge-distillation.html",
            "date": " ‚Ä¢ Nov 22, 2018"
        }
        
    
  
    
        ,"post4": {
            "title": "Machine Learning's Security Layer, an Overview",
            "content": "This is a shallow overview of the security of machine learning systems. Within a few scrolls we‚Äôll go through: . Ô∏èAdversarial Example | Model Theft | Dataset Poisoning | Dataset Protection | . Ô∏èAdversarial Examples . The adversarial examples (AE) topic is fascinating and an active area of research. It raises fundamental questions related to the limits and the security of our current gradient-based classifier architectures. AE are cleverly crafted data designed to be misclassified by a targeted model. They are ‚Äúdesigned to cause the model to make a mistake‚Äù (OpenAI, Attacking Machine Learning with Adversarial Examples). The image on the right is an adversarial example. . . The difference between the left and the rightmost dog is probably unperceptible. This can be due to our eyes limitations (or the bit depth of your monitor). And yet they are crucial to various models. The last image is indeed considered as a plane by a ResNet50 initialized with default training weight in Keras, and one AE will probably work on another architecture. The only difference are small pixels values, amplified in the second picture. . We can notice that ResNet50 was pretty confident that the dog on the left picture is a golden_retriever (~80%) and the crafted image is a plane with a higher confidence (~99%). So a model can be tricked into making a mistake with the confidence score we desire, we in general just need to train it long enough. What are the impacts of misclassifying with an arbitrary confidence score? . Recommendation systems are also studied for adversarial recommendation, influencing a recommendation system through indistinguishable fake users. . Safety . In most known models, any image can be crafted into another class with an arbitrary confidence score. So our dog can be misclassified as anything we wish with any arbitrary accuracy. It has been shown that it works in the physical world too, for instance, if we print them. A famous example is tricking a car‚Äôs sensor to see a speed limit instead of a STOP sign. The output of a model can be manipulated into making to some extent, a desired decision or at generating unhandled behavior by the application that relies on it. . . By the end of 2017 some showed that modifying one pixel can be enough in some cases. If you want to know more about this you can read the paper One pixel attack for fooling deep neural networks, enjoy a high-level presentation by the One minute paper channel or check this Keras implementation. . Adversarial examples are simple attack and don‚Äôt require much computation. On relatively small images a good GPU can craft an AE in less than a minute. This is a real security issue and that is probably why we can read those line at the end of some related subject papers: . Research was also supported in part by the Army Research Laboratory, under Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber Security CRA), and the Army Research Office under grant W911NF-13-1-0421. . Different threats level and techniques . We know that adversarial examples play with the decision boundaries of a classifier. We can, for instance, add random pixels on an image and change the classification or wisely choose those added pixels and choose the classification. Depending on the threat objective we denote: . Confidence reduction increases the ambiguity between classes by reducing the model‚Äôs confidence for a given image. | Misclassification changes the output class to another class than the original one. | Targeted misclassification forces the output of a specific input to be a specific target class. | . Depending on the opponent‚Äôs knowledge, there are three ways of crafting adversarial examples. Each with their own assumed prior knowledge of the target. Knowing: . the model as a whole including its weights (gradient-base), | only the score of each class (score-based), | only the prediction (transfer-based). | . A simplified copy of a diagram by Papernot. et al in The Limitations of Deep Learning in Adversarial Settings (page 3): . . One example of gradient-base attack consists in computing the loss‚Äô gradient function an image. Following by a tiny step in the opposite gradient‚Äôs direction. In order to keep valid RGB values, the image might be clipped between 0 and 255 and the value of the noise between 0 and a small value, M. This value M determine the maximum difference between the original image and the adversarial one, so M should be smaller than a human‚Äôs color sensibility (through a monitor). M smaller than 5 should be fine. The previous technique is called the iterative least-likely class method. Other types of gradient techniques exist like a fast gradient sign method. You can read this paper (part 2, page 3). We can note that they all require a complete knowledge of the model and its weights. . Score-based attacks rely only on the predicted model‚Äôs score to estimate the gradient and then apply the previous technique. Transfer-based attacks rely exclusively on the output label. This is a more realistic scenario compared to score-based and gradient-based. You can find an example of a transfer-based attack in the section Model Theft. . Defence . Here we won‚Äôt go much in depth I encourage you to search the keywords that attract you, it deserves a blog post on its own. We can see two big categories of defences: . Reactive: where the objective is an adversarial example prior being called on by our model for an inference. | Proactive: where the objective is to make the models more resilient to this kind of attack. Black Box Attacks by Nicolas Papernot et al. | . Example of reactive defenses: . MagNet a ‚Äòtwo networks‚Äô model composed of an auto-encoder capable of reforming before being fed to a classifier. Several auto-encoder are needed here so it‚Äôs resource expensive. | . Example of proactive defenses: . Random depthwise signed convolutional neural networks | Label smoothing (2016) | Mixup (2017) | Adversarial training, re-training the neural network with a subset of adversarial examples | Logit pairing this one is very new (2018) and ‚Äúachieves the state of the art defense for white box and black box attacks on ImageNet‚Äù | . Model Theft . Trying to rebuild someone‚Äôs else model or retrieve data that were used to train the model. The dataset and or the model might be confidential for their sensitive or commercial value. . The tension between model confidentiality and public access motivates our investigation of model extraction attacks. (Source) . We‚Äôll summarize briefly the Black Box Attacks by Nicolas Papernot et al. If you want to dig this subject you might enjoy reading it. The main idea described here is to create a local substitute neural network trained with a substitute dataset crafted by the adversary. Then, using gradient-based techniques adversarial examples can be generated. . . There‚Äôs no need for a labeled dataset, which can be expensive to produce. The substitute dataset is labeled using the remote DNN‚Äôs output. Then the local dataset is locally augmented through a technique called Jacobian-based Dataset Augmentation. Here is a pseudo code describing the Jacobian data augmentation (full code available on github). . def jacobian_augmentation(dataset): &quot;&quot;&quot; - get_label: API call on the remote oracle - alpha: step size - jacobian: returns jacobian matrix of the substitute model &quot;&quot;&quot; jacobian_dataset = [] for sample in dataset: label = get_label(sample) jacobian_sample = sample + alpha*sign(jacobian(substitute_model, label)) jacobian_dataset.append(jacobian_sample) return jacobian_dataset . Basically, each example is augmented by adding a small variation in direction of the gradient. . They emphasize that: . [‚Ä¶] this technique is not designed to maximize the substitute DNN‚Äôs accuracy but rather ensure that it approximates the oracle‚Äôs decision boundaries with few label queries. . The choice of architecture isn‚Äôt very important since we can assume some details beforehand. There is a high chance that a CNN was used for an image classification task. It is also possible to train simultaneously several architectures. . An implementation of a similar attack is available on Github. . Dataset Poisoning . Dataset poisoning attacks aim at manipulating model‚Äôs behavior at test time. . Poisoning 3% of a training set managed to drop the test accuracy by 11% (Certified Defenses for Data Poisoning Attacks by Steinhardt at al. (2017)). . Label flipping attack the objective is to maximize the loss function if a subset of the training example‚Äôs label is flipped, this is basically done by gradient ascent: . . An attacker first chooses a target instance from the test set; a successful poisoning attack causes this target example to be misclassified during test time. Next, the attacker samples a base instance from the base class, and makes imperceptible changes to it to craft a poison instance; this poison is injected into the training data with the intent of fooling the model into labeling the target instance with the base label at test time. Finally, the model is trained on the poisoned dataset (clean dataset + poison instances). If during test time the model mistakes the target instance as being in the base class, then the poisoning attack is considered successful Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks . Dataset Protection . Fully homomorphic encryption . . Fully homomorphic encryption is an encryption scheme that preserves the operation on data through encryption and decryption function. If the scheme is preserved over the addition, encrypting a sum or summing the encrypted members will give the same result. This means that you can encrypt your data locally and send it to a server, let it do a job using only the supported operators and return you the encrypted result. You don‚Äôt need to trust the server since it won‚Äôt understand what it is manipulating. . Let ENC and DEC the encryption and decryption function respectively: . ENC(X1 + X2) = ENC(X1) + ENC(X2) (homomorphism) Since X1 + X2 = DEC(ENC(X1+ X2)) We have X1 + X2 = DEC(ENC(X1) + ENC(X2)) . If you would need to follow one person in this field, it would be Craig Gentry. He found the first FHE scheme in 2009. . Much of Craig‚Äôs recent work, including FHE and cryptographic multilinear maps, generally falls into the area of ‚Äúlattice-based cryptography‚Äù. Unlike commonly-used cryptosystems like RSA and elliptic-curve cryptography, lattice-based cryptosystems cannot feasibly (as far as we know) be broken by quantum computers. (IBM) . The most important part here is that if one day this encryption schemes exists we can (almost) not care about the privacy of our data we‚Äôre sending on a remote machine. If this machine is malicious it can just give you wrong results but can‚Äôt exploit your data‚Ä¶ Unless‚Ä¶ If we‚Äôre talking about an FH encrypted machine learning model trying to predict something, nothing guarantees you that the model is empty at first and your opponent can still do inferences on the young model (by observing boundaries decisions and such). You should check out CryptoDL. . Dataset theft . It is also possible to recover the data used at training by simply looking at the model‚Äôs output, Membership Inference Attacks Against Machine Learning Models: . given a data record and black-box access to a model, determine if the record was in the model‚Äôs training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model‚Äôs predictions on the inputs that it trained on versus the inputs that it did not train on. . An implementation can be found here. . .",
            "url": "https://data-soup.github.io/blog/security/2018/10/02/machine-learning-security.html",
            "relUrl": "/security/2018/10/02/machine-learning-security.html",
            "date": " ‚Ä¢ Oct 2, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it‚Äôs in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://data-soup.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://data-soup.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}